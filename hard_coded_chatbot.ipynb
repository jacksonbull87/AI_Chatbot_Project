{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sof=pd.read_csv(\"df_python_stackoverflow .csv\")#.drop(columns='Unnamed: 0')\n",
    "# reddit=pd.read_csv(\"df_python_reddit.csv\")#[[\"question\",\"answer\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concate two dataframes \n",
    "# df=pd.concat([sof,reddit]).drop(columns='Unnamed: 0')\n",
    "# df=df.dropna(axis=0)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statistics analysis definition</td>\n",
       "      <td>Statistical analysis is the science of collect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is normal distribution</td>\n",
       "      <td>In probability theory, a normal (or Gaussian o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Definition of statistical distributions</td>\n",
       "      <td>A statistical distribution is a representation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Examples of Discrete Distributions</td>\n",
       "      <td>The Bernoulli Distribution\\nThe Bernoulli dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Examples of Continuous Distributions</td>\n",
       "      <td>A normal distribution is the single most impor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question  \\\n",
       "0           statistics analysis definition   \n",
       "1              what is normal distribution   \n",
       "2  Definition of statistical distributions   \n",
       "3      Examples of Discrete Distributions    \n",
       "4     Examples of Continuous Distributions   \n",
       "\n",
       "                                              answer  \n",
       "0  Statistical analysis is the science of collect...  \n",
       "1  In probability theory, a normal (or Gaussian o...  \n",
       "2  A statistical distribution is a representation...  \n",
       "3  The Bernoulli Distribution\\nThe Bernoulli dist...  \n",
       "4  A normal distribution is the single most impor...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"small_sample.csv\")[['question','answer']].dropna(axis=0).reset_index().drop(columns='index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]\\|', \"\", text)\n",
    "    text=text.replace(\"[\\'\",\"\").replace(\"\\n\",\" \").replace(\"']\",\" \").replace('[\"',\"\").replace('\"]',\"\").replace(\"it\\'s\",\"it's \").replace(\"\\', \\'\",\"\")\n",
    "    text=text.replace(\"\\',\",\"\").replace(\"it\\'s\",\"it is \").replace(\"it\\\\\\'s\",\"it is\").replace(\" \\\\\",\" \")\n",
    "    text=text.replace('\",',\" \").replace(\"\\',\",\"\").replace( \":\\',\",\"\").replace(\"here\\'s\",\"\").replace(\":\",\"\").replace(',\"',\"\")\n",
    "    text=text.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'s\",\"'s\")\n",
    "    text=re.sub(r\"let's\", \"let us\", text)\n",
    "    text = text.replace(\"\\'s\", \"\")\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#declare answers and questions\n",
    "questions=df.question\n",
    "answers=df.answer\n",
    "# Cleaning the questions\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "# Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['statistics analysis definition',\n",
       " 'what is normal distribution',\n",
       " 'definition of statistical distributions',\n",
       " 'examples of discrete distributions ',\n",
       " 'examples of continuous distributions',\n",
       " 'what is a probability mass function (pmf)?',\n",
       " 'definition of  continuous variables',\n",
       " 'what are continuous variables',\n",
       " 'definition of discrete variable',\n",
       " 'what is a discrete variable']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contains  punctuation\n",
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'said',\"'s\", \"also\",'one',\"n't\",'com', 'satirewire', '-', '–','--' ,\n",
    "'—', '_','satirewire.com']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in sw_set]# stop workds removal\n",
    "    return stopwords_removed\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "# import libraries\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    #tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    # stemming\n",
    "    stemming_string=list(map(stemming,processed_string))\n",
    "    # lemmatization\n",
    "    lemma_string=list(map(lemmatization,stemming_string))\n",
    "    \n",
    "    return lemma_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_questions=data_preprocessing(clean_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Greeting \n",
    "greeting=['hey', 'hi', 'hello', 'hey man', 'hi how are you', 'how’s it going', \n",
    "          'nice to meet you', 'how are you doing', 'what is up', 'what is new', \n",
    "          'what is going on', 'how is everything', 'how are things', 'how is life', \n",
    "          'how is your day', 'how is your day going', 'good to see you', 'nice to see you']\n",
    "goodbye=[\"see you\",\"bye\",\"byebye\",\"goodbye\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============# Hi There my name is Chacha. Let's Talk #=============\n",
      "You: Hi\n",
      "Chacha: Hey\n",
      "You: what is statistical analysis\n",
      "Chacha: Statistical analysis is the science of collecting data and uncovering patterns and trends. It’s really just another way of saying “statistics.” After collecting data you can analyze it to:\n",
      "Summarize the data. For example, make a pie chart.\n",
      "Find key measures of location. For example, the mean tells you what the average (or “middling”) number is in a set of data.\n",
      "Calculate measures of spread: these tell you if your data is tightly clustered or more spread out. The standard deviation is one of the more commonly used measures of spread; it tells you how spread out your data is about the mean.\n",
      "Make future predictions based on past behavior. This is especially useful in retail, manufacturing, banking, sports or for any organization where knowing future trends would be a benefit.\n",
      "Test an experiment’s hypothesis. Collecting data from an experiment only tells a story when you analyze the data. This part of statistical analysis is more formally called “Hypothesis Testing,” where the null hypothesis (the commonly accepted theory) is either proved or disproved. For more information please visit: https://www.statisticshowto.com/statistical-analysis/\n",
      "\n",
      "You: what is normal distribution\n",
      "Chacha: In probability theory, a normal (or Gaussian or Gauss or Laplace–Gauss) distribution is a type of continuous probability distribution for a real-valued random variable.  Source: https://en.wikipedia.org/wiki/Normal_distribution\n",
      "\n",
      "\n",
      "You: do you know how to plot distribution\n",
      "Chacha: you can use seaborn library- example:   \n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "sns.distplot(data.column,color=\"g\")\n",
      "\n",
      "You: what is numpy?\n",
      "Chacha: NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. For more information please visit : https://numpy.org/doc/stable/user/whatisnumpy.html\n",
      "\n",
      "You: how about pandas?\n",
      "Chacha: In computer programming, pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. For more information please visit the offical website.\n",
      "\n",
      "You: do you know what is data science?\n",
      "Chacha: Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms ... in managing a digital data collection. There is still no consensus on the definition of data science and it is considered by some to be a buzzword.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(\"=============# Hi There my name is Chacha. Let's Talk #=============\")\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    \n",
    "    question = input(\"You: \")\n",
    "    if question.lower() in goodbye:\n",
    "        print(\"Chacha: \"+ \"Bye\")\n",
    "        break \n",
    "    if  question.lower() in greeting:\n",
    "        print(\"Chacha: \"+ random.choice(greeting).title())\n",
    "        continue\n",
    "    # NLP\n",
    "    cleaned_question=clean_text(question)\n",
    "    processed_question=process_data(question)\n",
    "    stemming_question=stemming(processed_question)\n",
    "    lemma_question=lemmatization(stemming_question)\n",
    "    # to find which row has intersection with the words from question you asked\n",
    "    # which means to find the who have common elements between your question and the data\n",
    "    inter_list=[]\n",
    "    for i in cleaned_questions:\n",
    "        if (set(lemma_question) & set(i)):\n",
    "            inter_list.append((list(set(lemma_question) & set(i)),cleaned_questions.index(i)))\n",
    "    # find the max length of common elements\n",
    "    lengths=[len(inter_list[i][0]) for i in range(len(inter_list)) ]\n",
    "    indexes=[]\n",
    "    if len(lengths)>0:\n",
    "        # find all the index whose correspondiong question data have the most common elements\n",
    "        for i in range(len(inter_list)):\n",
    "            if len(inter_list[i][0])==max(lengths):\n",
    "                indexes.append(inter_list[i][1])  \n",
    "             # to randomly find an answer based on the index\n",
    "        answer_index=random.choice(indexes)\n",
    "        print(\"Chacha: \"+ answers[answer_index]+\"\\n\")\n",
    "    else:\n",
    "        print(\"Sorry, I don't know. I need to learn more!\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
