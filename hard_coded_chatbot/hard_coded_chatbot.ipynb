{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f1f91f568c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Packages which can be lazily imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/stem/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misri\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mISRIStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrslp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRSLPStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuffix_replace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_replace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/corpus/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyCorpusLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m abc = LazyCorpusLoader(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/corpus/reader/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \"\"\"\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaintext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCorpusReader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0mReader\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcorpora\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mconsist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mplaintext\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mParagraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/chatbot/lib/python3.5/site-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36mPlaintextCorpusReader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     def __init__(self, root, fileids,\n\u001b[1;32m     41\u001b[0m                  \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                  sent_tokenizer=nltk.data.LazyLoader(\n\u001b[0m\u001b[1;32m     43\u001b[0m                      'tokenizers/punkt/english.pickle'),\n\u001b[1;32m     44\u001b[0m                  \u001b[0mpara_block_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_blankline_block\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/yingyuxuan/Downloads/'\n",
    "df1=pd.read_csv(path+\"chatbot_data_Q&A - basic_python_questions.csv\").dropna(axis=0)\n",
    "df2=pd.read_csv(\"df_python_stackoverflow.csv\").drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df2,df1]).dropna(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    0\n",
       "answer      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the use of the yield keyword in Python...</td>\n",
       "      <td>To understand what yield does, you must unders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Given the following code, what does the if __n...</td>\n",
       "      <td>Whenever the Python interpreter reads a source...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If Python does not have a ternary conditional ...</td>\n",
       "      <td>Yes, it was added in version 2.5. The expressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Python, what are metaclasses and what do we...</td>\n",
       "      <td>A metaclass is the class of a class. A class d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do you call an external command (as if I'd...</td>\n",
       "      <td>Look at the subprocess module in the standard ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the use of the yield keyword in Python...   \n",
       "1  Given the following code, what does the if __n...   \n",
       "2  If Python does not have a ternary conditional ...   \n",
       "3  In Python, what are metaclasses and what do we...   \n",
       "4  How do you call an external command (as if I'd...   \n",
       "\n",
       "                                              answer  \n",
       "0  To understand what yield does, you must unders...  \n",
       "1  Whenever the Python interpreter reads a source...  \n",
       "2  Yes, it was added in version 2.5. The expressi...  \n",
       "3  A metaclass is the class of a class. A class d...  \n",
       "4  Look at the subprocess module in the standard ...  "
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222184, 2)"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some uncompleted questions\n",
    "questions=[]\n",
    "for i in df.question:\n",
    "    if i[-1]==\":\":\n",
    "        questions.append(None)\n",
    "    else:\n",
    "        questions.append(i)\n",
    "        \n",
    "        \n",
    "# remove some uncompleted answers\n",
    "answers=[]\n",
    "for i in df.answer:\n",
    "    if i[-1]==\":\":\n",
    "        answers.append(None)\n",
    "    else:\n",
    "        answers.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange a little bit\n",
    "df['answer']=answers\n",
    "df['question']=questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the use of the yield keyword in Python...</td>\n",
       "      <td>To understand what yield does, you must unders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Python, what are metaclasses and what do we...</td>\n",
       "      <td>A metaclass is the class of a class. A class d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What's the difference between the list methods...</td>\n",
       "      <td>append: Appends object at the end.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the difference between __str__ and __r...</td>\n",
       "      <td>Alex summarized well but, surprisingly, was to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>While using new_list = my_list, any modificati...</td>\n",
       "      <td>With new_list = my_list, you don't actually ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the use of the yield keyword in Python...   \n",
       "3   In Python, what are metaclasses and what do we...   \n",
       "12  What's the difference between the list methods...   \n",
       "14  What is the difference between __str__ and __r...   \n",
       "20  While using new_list = my_list, any modificati...   \n",
       "\n",
       "                                               answer  \n",
       "0   To understand what yield does, you must unders...  \n",
       "3   A metaclass is the class of a class. A class d...  \n",
       "12                 append: Appends object at the end.  \n",
       "14  Alex summarized well but, surprisingly, was to...  \n",
       "20  With new_list = my_list, you don't actually ha...  "
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(axis=0).copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119553, 2)"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]\\|', \"\", text)\n",
    "    text=text.replace(\"[\\'\",\"\").replace(\"\\n\",\" \").replace(\"']\",\" \").replace('[\"',\"\").replace('\"]',\"\").replace(\"it\\'s\",\"it's \").replace(\"\\', \\'\",\"\")\n",
    "    text=text.replace(\"\\',\",\"\").replace(\"it\\'s\",\"it is \").replace(\"it\\\\\\'s\",\"it is\").replace(\" \\\\\",\" \")\n",
    "    text=text.replace('\",',\" \").replace(\"\\',\",\"\").replace( \":\\',\",\"\").replace(\"here\\'s\",\"\").replace(\":\",\"\").replace(',\"',\"\")\n",
    "    text=text.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\'s\",\"'s\")\n",
    "    text=re.sub(r\"let's\", \"let us\", text)\n",
    "    text = text.replace(\"\\'s\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#declare answers and questions\n",
    "questions=df.question\n",
    "answers=df.answer\n",
    "# Cleaning the questions\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "# Cleaning the answers\n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definition of data visualization, what is data visualization?',\n",
       " 'what tools do professional data scientists use? ',\n",
       " 'mian tools in data science',\n",
       " 'how to install git',\n",
       " 'what is conda and anaconda?']"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_questions[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3e07921dc73b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# list contains  punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#sw_list = stopwords.words('english')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msw_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n\u001b[1;32m      5\u001b[0m \u001b[0;34m'said'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"also\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'one'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n't\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'com'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'–'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'--'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "# list contains  punctuation\n",
    "#sw_list = stopwords.words('english')\n",
    "sw_list = list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", '©',\n",
    "'said',\"'s\", \"also\",'one',\"n't\",'com', '-', '–','--' ,\n",
    "'—', '_']\n",
    "sw_set = set(sw_list)\n",
    "\n",
    "# tokenization\n",
    "def process_data(string):\n",
    "    tokens = nltk.word_tokenize(string) # tokenization\n",
    "    punctuation_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    return punctuation_removed\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "# create a function stemming() and loop through each word in a review\n",
    "def stemming(string):\n",
    "    stemmed_string=[]\n",
    "    for w in string:\n",
    "        stemmed_string.append(ps.stem(w))\n",
    "    return stemmed_string\n",
    "\n",
    "# import libraries\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create a function  and loop through each word in  a review\n",
    "def lemmatization(string):\n",
    "    lemma_list=[]\n",
    "    for word in string:\n",
    "        lemma_word=lemmatizer.lemmatize(word,pos='v') \n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Conbime all functions above and obtian cleaned text data \n",
    "def data_preprocessing(text_data):\n",
    "    #tokenization, stop words removal, punctuation marks removel\n",
    "    processed_string=list(map(process_data,text_data))\n",
    "    # stemming\n",
    "    stemming_string=list(map(stemming,processed_string))\n",
    "    # lemmatization\n",
    "    lemma_string=list(map(lemmatization,stemming_string))\n",
    "    \n",
    "    return lemma_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_questions=data_preprocessing(clean_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'be', 'the', 'use', 'of', 'the', 'yield', 'keyword', 'in', 'python', 'and', 'what', 'doe', 'it', 'do'], ['in', 'python', 'what', 'be', 'metaclass', 'and', 'what', 'do', 'we', 'use', 'them', 'for'], ['what', 'be', 'the', 'differ', 'between', 'the', 'list', 'method', 'append', 'and', 'extend'], ['what', 'be', 'the', 'differ', 'between', '__str__', 'and', '__repr__', 'in', 'python'], ['while', 'use', 'new_list', 'my_list', 'ani', 'modif', 'to', 'new_list', 'chang', 'my_list', 'everytim', 'whi', 'be', 'thi', 'and', 'how', 'can', 'i', 'clone', 'or', 'copi', 'the', 'list', 'to', 'prevent', 'it'], ['i', 'be', 'tri', 'to', 'understand', 'the', 'use', 'of', 'super', 'from', 'the', 'look', 'of', 'it', 'both', 'child', 'class', 'can', 'be', 'creat', 'just', 'fine'], ['pip', 'be', 'a', 'replac', 'for', 'easy_instal', 'but', 'should', 'i', 'instal', 'pip', 'use', 'easy_instal', 'on', 'window', 'be', 'there', 'a', 'better', 'way'], ['what', 'be', '__init__.pi', 'for', 'in', 'a', 'python', 'sourc', 'directori'], ['in', 'the', 'follow', 'method', 'definit', 'what', 'doe', 'the', 'and', '**', 'do', 'for', 'param2'], ['how', 'can', 'i', 'rais', 'an', 'except', 'in', 'python', 'so', 'that', 'it', 'can', 'later', 'be', 'catch', 'via', 'an', 'except', 'block']]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_questions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP(text):\n",
    "    cleaned_question=clean_text(text)\n",
    "    processed_question=process_data(cleaned_question)\n",
    "    stemming_question=stemming(processed_question)\n",
    "    lemma_question=lemmatization(stemming_question)\n",
    "    return lemma_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Greeting \n",
    "greeting=['hey', 'hi', 'hello', 'hey man', 'hi how are you', 'how are you','how is it going', \n",
    "          'nice to meet you', 'how are you doing', 'what is up', 'what is new', \n",
    "          'what is going on', 'how is everything', 'how are things', 'how is life', \n",
    "          'how is your day', 'how is your day going', 'good to see you', 'nice to see you']\n",
    "goodbye=[\"see you\",\"bye\",\"byebye\",\"goodbye\"]\n",
    "\n",
    "thankyou=[\"thanks\",\"thank you\", \"thank you very much\"]\n",
    "yourwelcome=[\"you are welcome ^.^\",\"my pleasure!\",\"I am happy to help you!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============# Hi There my name is Chacha. Let's Talk #=============\n",
      "You: hi\n",
      "Chacha: Hi\n",
      "\n",
      "You: how are you\n",
      "Chacha: Hey\n",
      "\n",
      "You: what does yield do in python?\n",
      "... ...\n",
      "Chacha: To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "print(\"=============# Hi There my name is Chacha. Let's Talk #=============\")\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    \n",
    "    question = input(\"You: \")\n",
    "    \n",
    "    if question.strip().lower() in goodbye:\n",
    "        print(\"Chacha: \"+ \"Bye\")\n",
    "        break \n",
    "        \n",
    "    if  question.strip().lower() in greeting:\n",
    "        print(\"Chacha: \"+ random.choice(greeting).capitalize()+\"\\n\")\n",
    "        continue\n",
    "        \n",
    "    if question.strip().lower() in thankyou:\n",
    "        print(\"Chacha: \"+ random.choice(yourwelcome).capitalize()+\"\\n\")\n",
    "        continue\n",
    "    # NLP\n",
    "    pro_text= NLP(question)\n",
    "\n",
    "    # to find which row has intersection with the words from question you asked\n",
    "    # which means to find the who have common elements between your question and the data\n",
    "    inter_list=[]\n",
    "    for i in cleaned_questions:\n",
    "        if (set(pro_text) & set(i)):\n",
    "            inter_list.append((list(set(pro_text) & set(i)),cleaned_questions.index(i)))\n",
    "\n",
    "    # remove stop words  \n",
    "    new_inter_list=[]\n",
    "    for i in range(len(inter_list)):\n",
    "        for j in inter_list[i][0]:\n",
    "            if j not in stopwords.words('english'):\n",
    "                new_inter_list.append(inter_list[i])\n",
    "\n",
    "    # find the max length of common elements\n",
    "    lengths=[len(new_inter_list[i][0]) for i in range(len(new_inter_list)) ]\n",
    "    max_length=max(lengths)\n",
    "\n",
    "\n",
    "    if len(lengths)>0:\n",
    "        indexes=[]\n",
    "        # find all the index whose correspondiong question data have the most common elements\n",
    "        for i in range(len(new_inter_list)):\n",
    "            if len(new_inter_list[i][0])==max_length:\n",
    "                indexes.append(new_inter_list[i][1])\n",
    "    else:\n",
    "        print(\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\")\n",
    "\n",
    "\n",
    "    ratios=[]\n",
    "    for i in list(set(indexes)):\n",
    "        ratio=len(pro_text)/len(questions.iloc[i])\n",
    "        ratios.append((ratio,i))\n",
    "   \n",
    "    max_ratios=max([ratios[i][0] for i in range(len(ratios))])\n",
    "\n",
    "    \n",
    "    final_indexes=[]\n",
    "    for i in range(len(ratios)):\n",
    "        if ratios[i][0]==max_ratios:\n",
    "            final_indexes.append(ratios[i][1])\n",
    "\n",
    "\n",
    "    if len(final_indexes)>0:\n",
    "        # to randomly find an answer based on the index\n",
    "        answer_index=random.choice(final_indexes)\n",
    "            \n",
    "        print(\"\\n\"+\"Chacha: \"+ answers.iloc[answer_index]+\"\\n\")\n",
    "    else:\n",
    "        print(\"Chacha: \"+\"Sorry, I don't know. I need to learn more!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_function(question):\n",
    "    # NLP\n",
    "    pro_text= NLP(question)\n",
    "\n",
    "    # to find which row has intersection with the words from question you asked\n",
    "    # which means to find the who have common elements between your question and the data\n",
    "    inter_list=[]\n",
    "    for i in cleaned_questions:\n",
    "        if (set(pro_text) & set(i)):\n",
    "            inter_list.append((list(set(pro_text) & set(i)),cleaned_questions.index(i)))\n",
    "\n",
    "    # remove stop words  \n",
    "    new_inter_list=[]\n",
    "    for i in range(len(inter_list)):\n",
    "        for j in inter_list[i][0]:\n",
    "            if j not in stopwords.words('english'):\n",
    "                new_inter_list.append(inter_list[i])\n",
    "\n",
    "    # find the max length of common elements\n",
    "    lengths=[len(new_inter_list[i][0]) for i in range(len(new_inter_list)) ]\n",
    "    max_length=max(lengths)\n",
    "\n",
    "\n",
    "    if len(lengths)>0:\n",
    "        indexes=[]\n",
    "        # find all the index whose correspondiong question data have the most common elements\n",
    "        for i in range(len(new_inter_list)):\n",
    "            if len(new_inter_list[i][0])==max_length:\n",
    "                indexes.append(new_inter_list[i][1])\n",
    "    else:\n",
    "        return \"Chacha: \"+\"Sorry, I don't know. I need to learn more!\"\n",
    "\n",
    "\n",
    "    ratios=[]\n",
    "    for i in list(set(indexes)):\n",
    "        ratio=len(pro_text)/len(questions.iloc[i])\n",
    "        ratios.append((ratio,i))\n",
    "   \n",
    "    max_ratios=max([ratios[i][0] for i in range(len(ratios))])\n",
    "\n",
    "    \n",
    "    final_indexes=[]\n",
    "    for i in range(len(ratios)):\n",
    "        if ratios[i][0]==max_ratios:\n",
    "            final_indexes.append(ratios[i][1])\n",
    "\n",
    "\n",
    "    if len(final_indexes)>0:\n",
    "        # to randomly find an answer based on the index\n",
    "        answer_index=random.choice(final_indexes)\n",
    "            \n",
    "        return \"Chacha: \"+ answers.iloc[answer_index]+\"\\n\"\n",
    "    else:\n",
    "        return \"Chacha: \"+\"Sorry, I don't know. I need to learn more!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1af82d38edd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"what is data sicence?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d989bab75a09>\u001b[0m in \u001b[0;36mnlp_function\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnlp_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# NLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpro_text\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# to find which row has intersection with the words from question you asked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NLP' is not defined"
     ]
    }
   ],
   "source": [
    "nlp_function(\"what is data sicence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
